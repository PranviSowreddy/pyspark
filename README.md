#  CS3.304 – Advanced Operating Systems  
## Assignment 4: Deadlock Detection using PySpark (MapReduce)

###  Objective
The goal of this assignment is to **implement a scalable deadlock detection system** in a distributed environment using **PySpark**.  
The algorithm analyzes a resource allocation log file to:
- Construct a **Resource Allocation Graph (RAG)**  
- Detect **deadlocks (cycles)** using iterative MapReduce  
- Perform **timeline and graph structure analysis**

---

### Environment Setup on ADA Cluster

#### 1 SSH into the Login Node
```bash
ssh <your_username>@ada.iiit.ac.in
```

#### 2 PySpark Environment Setup
Unlike a local environment, the ADA compute nodes do not have PySpark preinstalled.  
Hence, you must create and install it in a **virtual environment** using a SLURM job script.

#### 3 Create and Submit the Setup Script

Create a file named `pyspark_runner.sh` with the following content:

```bash
#!/bin/bash
#SBATCH --job-name=setup_pyspark_env
#SBATCH --partition=u22
#SBATCH --time=00:10:00
#SBATCH --mem=8G
#SBATCH --ntasks=1
#SBATCH --output=setup_pyspark_env.%j.out
#SBATCH --error=setup_pyspark_env.%j.err

echo "============================"
echo "Starting PySpark environment setup"
echo "============================"

# Step 1: Confirm Python version
python3 --version || { echo "Python3 not found"; exit 1; }

# Step 2: Create virtual environment if not exists
if [ ! -d "$HOME/pyspark_env" ]; then
    python3 -m venv $HOME/pyspark_env || { echo "Failed to create venv"; exit 1; }
else
    echo "Virtual environment already exists"
fi

# Step 3: Ensure pip and activate
$HOME/pyspark_env/bin/python -m ensurepip --upgrade
source $HOME/pyspark_env/bin/activate

# Step 4: Upgrade pip and install dependencies
python -m pip install --upgrade pip
python -m pip install pyspark==3.2.4 findspark

echo "============================"
echo "Setup complete!"
python -c "import pyspark; print('PySpark version:', pyspark.__version__)"
echo "============================"
```

Submit this setup job to the compute node:
```bash
sbatch pyspark_runner.sh
```

You can check job status:
```bash
squeue -u $USER
```

After the job completes, verify installation success:
```bash
cat setup_pyspark_env.<jobid>.out
```
You should see a **“SUCCESS”** message and PySpark version confirmation.

---

### Running the PySpark Job

After setting up the environment:

1. Upload your `.py` and `.log` files to the **login node**.
2. Create a new SLURM submission script named `aos_runner.sh`:

```bash
#!/bin/bash
#SBATCH --job-name=aos4
#SBATCH --partition=u22
#SBATCH --time=00:10:00
#SBATCH --mem=12G
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --output=aos4.%j.out
#SBATCH --error=aos4.%j.err

# Activate environment
source ~/pyspark_env/bin/activate

# Run Spark job
spark-submit AOS_Assignment4.py sample_hpc_rag.log 4
```

3. Submit this job to the compute node:
```bash
sbatch aos_runner.sh
```

4. Check job status:
```bash
squeue -u $USER
```

5. Once completed, view your results:
```bash
cat aos4.<jobid>.out
```

---

### Python Script Overview (`AOS_Assignment4.py`)

The main PySpark code performs:

| Section | Function | Description |
|----------|-----------|-------------|
| Q1.1 | `create_edge()` | Generates unique directed edges from WAIT/HOLD events. |
| Q1.2 | `graph_metrics()` | Computes number of process nodes, resource nodes, and edges. |
| Q2 | `cycle_detection()` | Detects cycles (deadlocks) using iterative MapReduce. |
| Q3.1 | `deadlock_timeline()` | Reports daily frequency of deadlock formations. |
| Q3.2 | Execution time | Measures time taken for cycle detection. |
| Q4 | `graph_structure_analysis()` | Identifies top 5 resources (by in-degree) and top 5 processes (by out-degree). |

All outputs are written sequentially into `outputs.txt` following the exact format required by the assignment.

---

###  Expected Output Format

```
99        # Total unique process nodes
49        # Total unique resource nodes
1638      # Total unique directed edges
107.74    # Execution time in seconds
5         # Total number of deadlock cycles
P102 R006 P159 R039 P143 R031
...
27        # Total number of unique processes involved
P102 P104 P105 ...
2025-11-02 2
2025-11-04 1
2025-11-06 2
R032 72
R024 72
R012 72
R036 72
R028 72
P154 22
P158 22
P131 22
P139 22
P167 22
```

---

### Files Included

| File | Description |
|------|--------------|
| `AOS_Assignment4.py` | PySpark code implementing RAG creation, cycle detection, and analysis |
| `pyspark_runner.sh` | Script to install and configure PySpark environment on ADA |
| `aos_runner.sh` | SLURM script to submit PySpark job |
| `sample_hpc_rag.log` | Example input log file |
| `outputs.txt` | Output generated by the PySpark script |
| `Report.pdf` | Observations, performance metrics, and scalability analysis |
| `README.md` | This documentation file |

---

###  Notes
- File names are **case-sensitive**; ensure they match exactly as mentioned in the scripts.
- Maximum job time limit: **10 minutes**
- Use `squeue -u $USER` to monitor submitted jobs.
- Always verify `.out` and `.err` files for job success or errors.


